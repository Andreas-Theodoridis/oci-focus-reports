# This is the full refactored Python module
# - Uses a staging table
# - Logs load metrics to a log table
# - Performs merge in database, not per row

import os
import gzip
import shutil
import logging
import oci
import csv
import json
import oracledb
import fnmatch
import base64
from datetime import datetime, timedelta, timezone

app_dir = "/home/opc/oci-focus-reports"

# Load config
config_dir = os.path.join(app_dir, "config")
with open(os.path.join(config_dir, "config.json")) as f:
    config = json.load(f)

rep_namespace = config["reporting_namespace"]
rep_bucket = config["reporting_bucket"]
dest_path = os.path.join(app_dir, "data", "fc")
app_dir = config["app_dir"]
log_file_pattern = config["focus_reports_file_name_pattern"]

focus_reports_table = config["focus_reports_table"]
db_user = config["db_user"]
db_pass = config["db_password"]
db_dsn = config["db_dsn"]
wallet_path = config["wallet_dir"]

# Logging setup
LOG_DIR = os.path.join(app_dir, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
OLD_LOG_DIR = os.path.join(LOG_DIR, "old")
os.makedirs(OLD_LOG_DIR, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = os.path.join(LOG_DIR, f"focus_reports_{timestamp}.log")
latest_log_symlink = os.path.join(LOG_DIR, "latest.log")

# Zip old logs
for filename in os.listdir(LOG_DIR):
    if (
        fnmatch.fnmatch(filename, log_file_pattern)
        and filename != os.path.basename(log_filename)
        and not filename.endswith(".gz")
    ):
        full_path = os.path.join(LOG_DIR, filename)
        gz_temp_path = full_path + ".gz"
        final_gz_path = os.path.join(OLD_LOG_DIR, os.path.basename(gz_temp_path))

        with open(full_path, "rb") as f_in, gzip.open(gz_temp_path, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)

        os.remove(full_path)
        shutil.move(gz_temp_path, final_gz_path)
        logging.info(f"📦 Archived log: {final_gz_path}")

# Remove logs older than 30 days
cutoff_date = datetime.now() - timedelta(days=30)
for filename in os.listdir(OLD_LOG_DIR):
    if filename.endswith(".gz"):
        full_path = os.path.join(OLD_LOG_DIR, filename)
        mtime = datetime.fromtimestamp(os.path.getmtime(full_path))
        if mtime < cutoff_date:
            os.remove(full_path)
            logging.info(f"🧹 Deleted old log archive: {filename}")

if os.path.exists(latest_log_symlink) or os.path.islink(latest_log_symlink):
    os.remove(latest_log_symlink)
os.symlink(os.path.basename(log_filename), latest_log_symlink)

logging.basicConfig(
    filename=log_filename,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

oracledb.init_oracle_client(lib_dir=config["oracle_client_lib_dir"])
os.makedirs(dest_path, exist_ok=True)


def try_parse_datetime(val, formats):
    for fmt in formats:
        try:
            return datetime.strptime(val, fmt)
        except ValueError:
            continue
    return None


def preprocess_row(header, row):
    new_row = []
    for col, val in zip(header, row):
        col = col.upper()
        val = val.strip()
        if not val:
            new_row.append(None)
            continue
        if col in ["BILLINGPERIODEND", "BILLINGPERIODSTART"]:
            new_row.append(try_parse_datetime(val, ["%Y-%m-%dT%H:%M:%S.%fZ"]))
        elif col in ["CHARGEPERIODEND", "CHARGEPERIODSTART"]:
            new_row.append(try_parse_datetime(val, ["%Y-%m-%dT%H:%MZ", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%dT%H:%M:%S.%fZ"]))
        else:
            new_row.append(val)
    return new_row


def create_stage_and_log_tables(cursor):
    logging.info("🔧 Checking and creating staging and log tables if needed...")
    cursor.execute("""
    DECLARE
        v_count INTEGER;
    BEGIN
        SELECT COUNT(*) INTO v_count FROM user_tables WHERE table_name = 'FOCUS_REPORTS_STAGE';
        IF v_count = 0 THEN
            EXECUTE IMMEDIATE 'CREATE TABLE FOCUS_REPORTS_STAGE AS SELECT * FROM FOCUS_REPORTS_PY WHERE 1=0';
            EXECUTE IMMEDIATE 'ALTER TABLE FOCUS_REPORTS_STAGE ADD (LOAD_DATE DATE DEFAULT SYSDATE, SOURCE_FILENAME VARCHAR2(500))';
        END IF;

        SELECT COUNT(*) INTO v_count FROM user_tables WHERE table_name = 'FOCUS_REPORTS_LOAD_LOG';
        IF v_count = 0 THEN
            EXECUTE IMMEDIATE '
                CREATE TABLE FOCUS_REPORTS_LOAD_LOG (
                    ID NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
                    FILENAME VARCHAR2(500),
                    LOAD_DATE TIMESTAMP DEFAULT SYSTIMESTAMP,
                    TOTAL_ROWS NUMBER,
                    FAILED_ROWS NUMBER,
                    UPDATED_ROWS NUMBER,
                    STATUS VARCHAR2(20),
                    ERROR_MESSAGE VARCHAR2(4000)
                )';
        END IF;
    END;
    """)


def insert_into_stage(csv_path, connection, source_file):
    cursor = connection.cursor()
    logging.info(f"📄 Parsing CSV file: {csv_path}")
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = [h.strip().upper() for h in next(reader)]
        insert_sql = f"""
            INSERT INTO FOCUS_REPORTS_STAGE ({', '.join(f'"{h}"' for h in header)}, SOURCE_FILENAME)
            VALUES ({', '.join([f':{i+1}' for i in range(len(header))])}, :{len(header)+1})
        """
        total, inserted, failed = 0, 0, 0
        for row in reader:
            if not any(row):
                continue
            total += 1
            try:
                cleaned = preprocess_row(header, row)
                cursor.execute(insert_sql, cleaned + [source_file])
                inserted += 1
            except Exception as row_err:
                logging.warning(f"⚠️ Failed row in {csv_path}: {row_err}")
                failed += 1
        connection.commit()
        cursor.close()
        return total, inserted, failed


def log_load(cursor, filename, total, inserted, updated, status, error=None):
    cursor.execute("""
        INSERT INTO FOCUS_REPORTS_LOAD_LOG (
            FILENAME, TOTAL_ROWS, INSERTED_ROWS, FAILED_ROWS, STATUS, ERROR_MESSAGE
        ) VALUES (:1, :2, :3, :4, :5, :6)
    """, (filename, total, inserted, updated, status, error))


def log_and_execute(cursor, sql, params=None):
    if params:
        logging.info(f"➡️ Executing SQL:\n{sql}\nWith params: {params}")
        cursor.execute(sql, params)
    else:
        logging.info(f"➡️ Executing SQL:\n{sql}")
        cursor.execute(sql)


def get_secret_value(secret_ocid, signer):
    secret_client = oci.secrets.SecretsClient(config={}, signer=signer)
    response = secret_client.get_secret_bundle(secret_id=secret_ocid)
    base64_secret = response.data.secret_bundle_content.content
    secret_value = base64.b64decode(base64_secret).decode("utf-8")
    return secret_value

def get_columns(cursor, table_name):
    cursor.execute(f"SELECT column_name FROM user_tab_columns WHERE table_name = '{table_name.upper()}' ORDER BY column_id")
    return [row[0] for row in cursor.fetchall()]

def download_extract_and_insert():
    try:
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        object_storage = oci.object_storage.ObjectStorageClient(config={}, signer=signer)

        db_password = db_pass
        if "pass_secret_ocid" in config.get("db_credentials", {}):
            db_password = get_secret_value(config["db_credentials"]["pass_secret_ocid"], signer)

        if config.get("use_dynamic_prefix", False):
            days_back = config.get("days_back", 1)  # Default to 1 if not defined
            target_day = datetime.now(timezone.utc) - timedelta(days=days_back)
            base = config.get("prefix_base", "FOCUS Reports")
            prefix = f"{base}/{target_day.year}/{target_day.strftime('%m')}/{target_day.strftime('%d')}"
            cutoff_time = datetime(target_day.year, target_day.month, target_day.day)
        else:
            prefix = config["prefix_file"]
            cutoff_time = None

        report_bucket_objects = oci.pagination.list_call_get_all_results(
            object_storage.list_objects,
            namespace_name=rep_namespace,
            bucket_name=rep_bucket,
            prefix=prefix,
            fields='timeCreated'
        )

        valid_objects = [
            obj for obj in report_bucket_objects.data.objects
            if not cutoff_time or obj.time_created.replace(tzinfo=None) >= cutoff_time
        ]

        os.environ["TNS_ADMIN"] = wallet_path

        with oracledb.connect(user=db_user, password=db_password, dsn=db_dsn) as conn:
            cursor = conn.cursor()
            create_stage_and_log_tables(cursor)

            logging.info("🧹 Truncating staging table before loading files...")
            cursor.execute("TRUNCATE TABLE FOCUS_REPORTS_STAGE")

            total_rows_staged = 0

            for obj in valid_objects:
                local_gz_path = os.path.join(dest_path, obj.name)
                extracted_path = local_gz_path.rstrip(".gz")
                os.makedirs(os.path.dirname(local_gz_path), exist_ok=True)

                # Download
                response = object_storage.get_object(rep_namespace, rep_bucket, obj.name)
                with open(local_gz_path, "wb") as f:
                    for chunk in response.data.raw.stream(1024 * 1024, decode_content=False):
                        f.write(chunk)
                logging.info(f"📥 Downloaded: {obj.name}")

                # Extract
                with gzip.open(local_gz_path, "rb") as gz_file:
                    with open(extracted_path, "wb") as out_file:
                        shutil.copyfileobj(gz_file, out_file)
                os.remove(local_gz_path)
                logging.info(f"📂 Extracted to: {extracted_path}")

                try:
                    total, inserted, failed = insert_into_stage(extracted_path, conn, obj.name)
                    total_rows_staged += inserted
                    logging.info(f"📊 {obj.name}: Total={total}, ✅ Inserted={inserted}, ❌ Failed={failed}")
                    log_load(cursor, obj.name, total, inserted, 0, "STAGED")
                except Exception as e:
                    logging.error(f"❌ Failed to insert file {obj.name}: {str(e)}")
                    conn.rollback()
                    log_load(cursor, obj.name, 0, 0, 0, "FAILED", str(e))

            # Now update the main table
            logging.info("🗑️ Deleting matching rows from main table based on staging data...")
            cursor.execute(f"""
                DELETE FROM {focus_reports_table}
                WHERE oci_referencenumber IN (
                    SELECT DISTINCT oci_referencenumber FROM FOCUS_REPORTS_STAGE
                )
            """)

            logging.info("➕ Inserting data from staging into main table...")
            # Explicitly list columns to match the main table
            cursor.execute(f"""
                INSERT INTO {focus_reports_table} (
                    {', '.join(get_columns(cursor, focus_reports_table))}
                )
                SELECT {', '.join(get_columns(cursor, focus_reports_table))}
                FROM FOCUS_REPORTS_STAGE
            """)

            conn.commit()
            logging.info(f"✅ Main table updated with {total_rows_staged} total staged rows.")

        # Run post-processing procedures
        with oracledb.connect(user=db_user, password=db_password, dsn=db_dsn) as final_conn:
            final_cursor = final_conn.cursor()
            log_and_execute(final_cursor, "BEGIN PAGE1_CONS_WRKLD_MONTH_CHART_DATA_PROC; END;")
            log_and_execute(final_cursor, "BEGIN PAGE1_CONS_WRKLD_WEEK_CHART_DATA_PROC; END;")
            log_and_execute(final_cursor, "BEGIN REFRESH_COST_USAGE_TS_PROC; END;")
            log_and_execute(final_cursor, "BEGIN REFRESH_CREDIT_USAGE_AGG_PROC; END;")
            log_and_execute(final_cursor, "BEGIN REFRESH_CREDIT_CONSUMPTION_STATE_PROC; END;")
            log_and_execute(final_cursor, "BEGIN DBMS_MVIEW.REFRESH('FILTER_VALUES_MV', METHOD => 'C'); END;")
            final_conn.commit()
            final_cursor.close()

    except Exception as e:
        logging.error(f"❌ Global failure: {e}")


if __name__ == "__main__":
    download_extract_and_insert()
